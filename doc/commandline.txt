The important thing to always bear in mind when dealing with data with the command line is that the details_* files and top.json are used to generate most of the other data files (alt_*, cal_*, csv_*, report_*, student_*). The only files which are not generated from top.json and details_* are files which don't directly relate to the timetable data (only user_* at the moment, I think).

So the usual strategy for making any changes is first to get details_* and top.json correct and then force a regeneration of all the other files using indium.py -r. It's never wrong to run -r (except on a live system, given how attrocious our locking is) to regererate everything as it should be a no-op  if nothing has changed.

-r is pretty much the only command you should run out of indium.py. All the rest of what's done from the command line is about getting the data into top.json and details_* in the first place. (The other options are just run at various times by the PHP to resync data).

Only two other scripts need to be run to handle updates. These sit inside the directory "generator" and need to be run with caution. The problem arises from these being written initially to populate data from test files and not having been adapted to there being live data in the system. Therefore, for one thing, they should never be run on any server, but only on dev boxes. For another, once run, the files generated should be carefully picked over, so only the ones deliberately being updated go onto the server.

top.json is generated by topgen.py, which uses various spreadsheets, which are checked into SVN, and it should be clear from the source which they are. It should always be safe to update top.json.

The more dangerous script is detgen.py. This generates details_* files from example and test data and top.json (as generated by topgen.py). This includes the details files for courses which have only supplied a link to us (they have stub details_* files). When this script is run, carefully copy the ones you want out of the generated data, to put into a directory with good data for the other courses.

In both cases, I always run a JSON diff at the moment, to check changes, before uploading them.

The reasons you might want to run this unruly pair are:
  i. courses added, deleted or renamed;
  ii. links to timetables changed;
  iii. courses transitioning between links to full timetables.
  
In each case the changes are restricted to a particular course code, and can be updated from the various csv's (all in SVN) used to generate them. I then look in top.json for the coursecode (T..........) and copy the top and details_T..... file to check changes, discarding the rest.

The only non-obvious part of the .csvs is probably idmap.csv, which caches IDs, and so makes them stable, to rows in the main course spreadsheet, which is identified only by title. If a course moves faculty or changes name, continuity can be assured by manual editing of this csv.

To transition away from a PDF to a full timetable, just remove the references to it in pdfs.csv (& possibly related files, should be clear from the topgen.py and detgen.py code) and detgen should generate an example full timetable. You can then upload that data and upload the correct data using the spreadsheet upload facility in the user interface.

The other scripts in that directory are from various data feeds we pulled data from. There are no automatic inbound feeds at the moment. pdn*.py relates to Christof's system. This essentially works, but there are issues with currency etc of Christof's system and the way SBS organise their courses to rotate between departments. If we can be certain that his data is good, we could get these going quite easily. ygen.py relates to Yseult Jay's old system. They much prefer editing through the UI, so there's probably no need to run this again.

